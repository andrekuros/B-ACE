{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from typing import Optional, Tuple\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "import pandas as pd \n",
    "\n",
    "import random\n",
    "\n",
    "from torch.distributions import Normal, Distribution\n",
    "import pandas as pd\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import SubprocVectorEnv, DummyVectorEnv\n",
    "#from tianshou.env.pettingzoo_env_parallel import PettingZooParallelEnv\n",
    "#from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "#from PettingZooParallelEnv import PettingZooParal\n",
    "\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "\n",
    "from tianshou.utils.net.common import ActorCritic, DataParallelNet, Net\n",
    "from tianshou.utils.net.continuous import Actor, Critic\n",
    "\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, DDPGPolicy\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from DNN_B_ACE_ACTOR import DNN_B_ACE_ACTOR\n",
    "from DNN_B_ACE_CRITIC import DNN_B_ACE_CRITIC\n",
    "from Task_MHA_B_ACE import Task_MHA_B_ACE\n",
    "from Task_DNN_B_ACE import Task_DNN_B_ACE\n",
    "from Task_B_ACE_Env import B_ACE_TaskEnv\n",
    "\n",
    "from CollectorMA import CollectorMA\n",
    "from MAParalellPolicy import MAParalellPolicy\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "\n",
    "####---------------------------#######\n",
    "#Tianshou Adjustment\n",
    "import wandb\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Tianshow_Training_GoDot.ipybn\"\n",
    "from tianshou.utils import WandbLogger\n",
    "# from tianshou.utils.logger.base import LOG_DATA_TYPE\n",
    "# def new_write(self, step_type: str, step: int, data: LOG_DATA_TYPE) -> None:\n",
    "#      data[step_type] = step\n",
    "#      wandb.log(data)   \n",
    "# WandbLogger.write = new_write \n",
    "####---------------------------#######\n",
    "\n",
    "\n",
    "model  =  \"Task_MHA_B_ACE\"#\"SISL_Task_MultiHead\" #\"CNN_ATT_SISL\" #\"MultiHead_SISL\" \n",
    "test_num  =  \"_B_ACE03\"\n",
    "policyModel  =  \"DQN\"\n",
    "name = model + test_num\n",
    "\n",
    "train_env_num = 4\n",
    "test_env_num  = 15\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = name + str(now)\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn_sisl\", log_name)\n",
    "\n",
    "#Duck Training Best\n",
    "#load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE02240721-151049_1261_BestRew.pth'\n",
    "\n",
    "#FSMG1 Training Best\n",
    "load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241019-215530_1893_BestRew.pth'\n",
    "\n",
    "save_policy_name = f'policy_{log_name}'\n",
    "policy_path = model + policyModel\n",
    "\n",
    "\n",
    "model_load_path = os.path.join(policy_path, load_policy_name)  \n",
    "model_save_path = os.path.join(policy_path, save_policy_name)        \n",
    "os.makedirs(os.path.join(policy_path), exist_ok=True)  \n",
    "os.makedirs(os.path.join(log_path), exist_ok=True)\n",
    "\n",
    "Policy_Config = {\n",
    "    \"same_policy\" : True,\n",
    "    \"load_model\" : False,\n",
    "    \"freeze_CNN\" : False     \n",
    "                }\n",
    "\n",
    "B_ACE_Config = { \t\n",
    "                    \"EnvConfig\" : \n",
    "                    {\n",
    "                        \"task\": \"b_ace_v1\",\n",
    "                        \"env_path\": \"../../BVR_AirCombat/bin/B_ACE_v13.exe\",\n",
    "                        \"port\": 12500,\n",
    "                        \"renderize\": 1,                        \n",
    "                        \"phy_fps\": 20,\n",
    "                        \"speed_up\": 10,\n",
    "                        \"max_cycles\": 36000,\n",
    "                        \"experiment_mode\"  : 0,\n",
    "                        \"parallel_envs\": 1,\t\n",
    "                        \"seed\": 1,\t\n",
    "                        \"action_repeat\": 20,\t\n",
    "                        \"action_type\": \"Low_Level_Continuous\",                        \n",
    "                        \"stop_mission\" : 1,\n",
    "                        \"max_trail_size\" : 240,\n",
    "                        \n",
    "                        \"additional_config\": [\"\"],\n",
    "                                                \n",
    "                        \"RewardsConfig\" : {\n",
    "                                    \"mission_factor\": 0.001,\t\t\t\t\n",
    "                                    \"missile_fire_factor\": -0.1,\t\t\n",
    "                                    \"missile_no_fire_factor\": -0.001,\n",
    "                                    \"missile_miss_factor\": -0.5,\n",
    "                                    \"detect_loss_factor\": -0.1,\n",
    "                                    \"keep_track_factor\": 0.001,\n",
    "                                    \"hit_enemy_factor\": 3.0,\n",
    "                                    \"hit_own_factor\": -5.0,\t\t\t\n",
    "                                    \"mission_accomplished_factor\": 10.0,\t\t\t\n",
    "                                }\n",
    "                    },\n",
    "\n",
    "                    \"AgentsConfig\" : \n",
    "                    {\n",
    "                        \"blue_agents\": { \n",
    "                            \"num_agents\" : 1,\n",
    "                            \"share_states\" : 1, \n",
    "                            \"share_tracks\" : 1,\n",
    "                            \"mission\"    : \"DCA\",\n",
    "                            \"beh_config\" : {\n",
    "                                            \"dShot\" : [1.04, 0.50, 1.09],\n",
    "                                            \"lCrank\": [1.06, 0.98, 0.98],\n",
    "                                            \"lBreak\": [1.05, 1.17, 0.45]\n",
    "                                        },\n",
    "                            \"base_behavior\": \"external\",                  \n",
    "                            \"init_position\": {\"x\": 0.0, \"y\": 25000.0,\"z\": 30.0},\n",
    "                            \"offset_pos\": {\t\"x\": 0.0, \"y\": 0.0, \"z\": 0.0},\n",
    "                            \"init_hdg\": 0.0,                        \n",
    "                            \"target_position\": {\"x\": 0.0,\"y\": 25000.0,\"z\": 30.0},\n",
    "                            \"rnd_offset_range\":{\"x\": 10.0,\"y\": 10000.0,\"z\": 5.0},\t\t\t\t\n",
    "                            \"rnd_shot_dist_var\": 0.025,\n",
    "                            \"rnd_crank_var\": 0.025,\n",
    "                            \"rnd_break_var\": 0.025,\n",
    "                            \"wez_models\" : \"res://assets/wez/Default_Wez_params.json\"\n",
    "                        },\t\n",
    "                        \"red_agents\":\n",
    "                        { \n",
    "                            \"num_agents\" : 1,\n",
    "                            \"share_states\" : 1, \n",
    "                            \"share_tracks\" : 1, \n",
    "                            \"base_behavior\": \"duck\",\n",
    "                            \"mission\"    : \"striker\",\n",
    "                            \"beh_config\" : {\n",
    "                                              \"dShot\" : [1.04, 1.04, 1.04], #[1.04, 0.50, 1.09]\n",
    "                                              \"lCrank\": [1.06, 1.06, 1.06], #1.06, 0.98, 0.98\n",
    "                                              \"lBreak\": [1.05, 1.05, 1.05] #1.05, 1.17, 0.45\n",
    "                                          },\n",
    "                            # \"beh_config\" : {\n",
    "                            #                \"dShot\" : [1.04, 0.50, 1.09],\n",
    "                            #                 \"lCrank\": [1.06, 0.98, 0.98],\n",
    "                            #                 \"lBreak\": [1.05, 1.17, 0.45]\n",
    "                            #             },\n",
    "                            \"init_position\": {\"x\": 0.0,\"y\": 25000.0,\"z\": -30.0},\n",
    "                            \"offset_pos\": {\"x\": 0.0,\"y\": 0.0,\"z\": 0.0},\n",
    "                            \"init_hdg\" : 180.0,                        \n",
    "                            \"target_position\": {\"x\": 0.0,\"y\": 25000.0,\"z\": 30.0},\n",
    "                            \"rnd_offset_range\":{\"x\": 25.0,\"y\": 10000.0,\"z\": 10.0},\t\t\t\t\n",
    "                            \"rnd_shot_dist_var\": 0.025,\n",
    "                            \"rnd_crank_var\": 0.025,\n",
    "                            \"rnd_break_var\": 0.025,\n",
    "                            \"wez_models\" : \"res://assets/wez/Default_Wez_params.json\"\n",
    "                        }\n",
    "                    }\t\n",
    "            }\n",
    "#max_cycles = B_ACE_Config[\"max_cycles\"]\n",
    "#n_agents = 1#B_ACE_Config[\"n_pursuers\"]\n",
    "\n",
    "dqn_params =    {\n",
    "                \"discount_factor\": 0.99, \n",
    "                \"estimation_step\": 180, \n",
    "                \"target_update_freq\": 6000 * 3 ,#max_cycles * n_agents,\n",
    "                \"reward_normalization\" : False,\n",
    "                \"clip_loss_grad\" : False,\n",
    "                \"optminizer\": \"Adam\",\n",
    "                \"lr\": 0.00005, \n",
    "                \"max_tasks\" : 30\n",
    "                }\n",
    "\n",
    "PPO_params= {    \n",
    "                'action_scaling': True,\n",
    "                'discount_factor': 0.98,\n",
    "                'max_grad_norm': 0.5,\n",
    "                'eps_clip': 0.2,\n",
    "                'vf_coef': 0.5,\n",
    "                'ent_coef': 0.01,\n",
    "                'gae_lambda': 0.95,\n",
    "                'reward_normalization': False, \n",
    "                'dual_clip': None,\n",
    "                'value_clip': False,   \n",
    "                'deterministic_eval': True,\n",
    "                'advantage_normalization': False,\n",
    "                'recompute_advantage': False,\n",
    "                'action_bound_method': \"clip\",\n",
    "                'lr_scheduler': None,\n",
    "            }\n",
    "\n",
    "\n",
    "trainer_params = {\"max_epoch\": 500,\n",
    "                  \"step_per_epoch\": 18000,#5 * (150 * n_agents),\n",
    "                  \"step_per_collect\": 6000,# * (10 * n_agents),\n",
    "                  \n",
    "                  \"batch_size\" : 1024,\n",
    "                  \n",
    "                  \"update_per_step\": 1 / (100), #Off-Policy Only (run after close a Collect (run many times as necessary to meet the value))\n",
    "                  \n",
    "                  \"repeat_per_collect\": 32, #On-Policy Only\n",
    "                  \n",
    "                  \"episode_per_test\": 30,                  \n",
    "                  \"tn_eps_max\": 0.20,\n",
    "                  \"ts_eps_max\": 0.01,\n",
    "                  \"warmup_size\" : 1,\n",
    "                  \"train_envs\" : train_env_num,\n",
    "                  \"test_envs\" : test_env_num\n",
    "}\n",
    "#agent_learn = PPOPolicy(**policy_params)\n",
    "\n",
    "runConfig = dqn_params\n",
    "runConfig.update(Policy_Config)\n",
    "runConfig.update(B_ACE_Config)\n",
    "runConfig.update(trainer_params) \n",
    "runConfig.update(dqn_params)\n",
    "\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    "    policy_load_path = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \n",
    "    env = _get_env()       \n",
    "    agent_observation_space = env.observation_space(\"agent_0\")\n",
    "\n",
    "    #print(env.action_space)\n",
    "    #action_shape = 50#env.action_space.shape\n",
    "    \n",
    "    #print(\"ActionSPACE: \", env.action_space)\n",
    "    action_space = env.action_space\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"  \n",
    "\n",
    "    agents = []        \n",
    "    \n",
    "    if Policy_Config[\"same_policy\"]:\n",
    "        policies_number = 1\n",
    "    else:\n",
    "        policies_number = len(env.agents)\n",
    "\n",
    "    for _ in range(policies_number):      \n",
    "        \n",
    "        #print(agent_observation_space)\n",
    "        \n",
    "        if policyModel == \"DQN\":\n",
    "\n",
    "            if model == \"Task_MHA_B_ACE\":\n",
    "                net = Task_MHA_B_ACE(\n",
    "                    #obs_shape=agent_observation_space.shape,                                                  \n",
    "                    num_tasks = dqn_params[\"max_tasks\"],\n",
    "                    num_features_per_task= 14,                    \n",
    "                    nhead = 4,\n",
    "                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                    \n",
    "                ).to(device) \n",
    "            \n",
    "            if model == \"Task_DNN_B_ACE\":\n",
    "                net = Task_DNN_B_ACE(\n",
    "                    #obs_shape=agent_observation_space.shape,                                                  \n",
    "                    num_tasks = dqn_params[\"max_tasks\"],\n",
    "                    num_features_per_task= 14,                    \n",
    "                    nhead = 4,\n",
    "                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                    \n",
    "                ).to(device) \n",
    "                \n",
    "            optim = torch.optim.Adam(net.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True)       \n",
    "            \n",
    "            agent_learn = DQNPolicy(\n",
    "                model=net,\n",
    "                optim=optim,\n",
    "                action_space = Discrete(dqn_params[\"max_tasks\"]),\n",
    "                discount_factor= dqn_params[\"discount_factor\"],\n",
    "                estimation_step=dqn_params[\"estimation_step\"],\n",
    "                target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "                reward_normalization = dqn_params[\"reward_normalization\"],\n",
    "                clip_loss_grad = dqn_params[\"clip_loss_grad\"]\n",
    "            )                   \n",
    "        \n",
    "        elif model == \"PPO_DNN\":\n",
    "            \n",
    "            actor = DNN_B_ACE_ACTOR(\n",
    "                obs_shape=agent_observation_space.shape[0],                \n",
    "                action_shape=4,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"                \n",
    "            ).to(device)\n",
    "\n",
    "            critic = DNN_B_ACE_CRITIC(\n",
    "                obs_shape=agent_observation_space.shape[0],                \n",
    "                action_shape=4,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"                \n",
    "            ).to(device)\n",
    "            \n",
    "                                    \n",
    "            actor_critic = ActorCritic(actor, critic)\n",
    "        \n",
    "            # orthogonal initialization\n",
    "            # for m in actor_critic.modules():\n",
    "            #     if isinstance(m, torch.nn.Linear):\n",
    "            #         torch.nn.init.orthogonal_(m.weight)\n",
    "            #         torch.nn.init.zeros_(m.bias)            \n",
    "            \n",
    "            # dist = torch.distributions.Normal(torch.tensor([0.0]), torch.tensor([1.0])) \n",
    "                # define policy\n",
    "            def dist(mu, sigma) -> Distribution:\n",
    "                return Normal(mu, sigma)        \n",
    "                \n",
    "            #optim_actor  = torch.optim.Adam(netActor.parameters(),  lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "            #optim_critic = torch.optim.Adam(netCritic.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "            optim = torch.optim.Adam(actor_critic.parameters(), lr=dqn_params[\"lr\"])\n",
    "                    \n",
    "            agent_learn = PPOPolicy(\n",
    "                actor=actor,\n",
    "                critic=critic,\n",
    "                optim=optim,\n",
    "                dist_fn=dist,                \n",
    "                action_scaling  =       PPO_params['action_scaling'],\n",
    "                discount_factor =       PPO_params['discount_factor'],\n",
    "                max_grad_norm   =       PPO_params['max_grad_norm'],\n",
    "                eps_clip        =       PPO_params['eps_clip'],\n",
    "                vf_coef         =       PPO_params['vf_coef'],\n",
    "                ent_coef        =       PPO_params['ent_coef'],\n",
    "                gae_lambda      =       PPO_params['gae_lambda'],\n",
    "                reward_normalization=   PPO_params['reward_normalization'],\n",
    "                action_space    =  action_space,\n",
    "                deterministic_eval=     PPO_params['deterministic_eval'],\n",
    "                advantage_normalization=PPO_params['advantage_normalization'],\n",
    "                recompute_advantage=    PPO_params['recompute_advantage'],\n",
    "                action_bound_method=    PPO_params['action_bound_method'],\n",
    "                lr_scheduler=None\n",
    "            )\n",
    "            \n",
    "        if Policy_Config[\"load_model\"] is True:\n",
    "            # Load the saved checkpoint             \n",
    "            agent_learn.load_state_dict(torch.load(model_load_path))\n",
    "            print(f'Loaded-> {model_load_path}')                   \n",
    "        \n",
    "        agents.append(agent_learn)\n",
    "\n",
    "    if Policy_Config[\"same_policy\"]:\n",
    "        agents = [agents[0] for _ in range(len(env.agents))]\n",
    "    else:\n",
    "        for _ in range(len(env.agents) - policies_number):\n",
    "            agents.append(agents[0])\n",
    "    \n",
    "    policy = MultiAgentPolicyManager(policies = agents, env=env)  \n",
    "    #policy = MAParalellPolicy(policies = agents, env=env, device=\"cuda\" if torch.cuda.is_available() else \"cpu\" )  \n",
    "        \n",
    "    return policy, optim, env.agents\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"   \n",
    "    \n",
    "    B_ACE_Config[\"EnvConfig\"][\"seed\"] = random.randint(0, 1000000)\n",
    "    \n",
    "    env = B_ACE_TaskEnv( convert_action_space = True,\n",
    "                                    device = \"cpu\",\n",
    "                                    **B_ACE_Config)\n",
    "    \n",
    "    #env.action_space = env.action_space()\n",
    "    #env = PettingZooEnv(env)  \n",
    "    \n",
    "    return env \n",
    "\n",
    "# Function to update the red agent's behavior in the B_ACE_Config dictionary\n",
    "def update_red_agent_behavior(config, blue_agents_num, red_agents_num, base_behavior, mission, dShot, lCrank, lBreak):\n",
    "    config[\"AgentsConfig\"][\"blue_agents\"][\"num_agents\"] = blue_agents_num\n",
    "    config[\"AgentsConfig\"][\"red_agents\"][\"num_agents\"] = red_agents_num\n",
    "    \n",
    "    config[\"AgentsConfig\"][\"red_agents\"][\"base_behavior\"] = base_behavior\n",
    "    config[\"AgentsConfig\"][\"red_agents\"][\"mission\"] = mission\n",
    "    config[\"AgentsConfig\"][\"red_agents\"][\"beh_config\"][\"dShot\"] = dShot\n",
    "    config[\"AgentsConfig\"][\"red_agents\"][\"beh_config\"][\"lCrank\"] = lCrank\n",
    "    config[\"AgentsConfig\"][\"red_agents\"][\"beh_config\"][\"lBreak\"] = lBreak\n",
    "    \n",
    "\n",
    "# Function to compute mean and 95% bootstrap confidence interval\n",
    "def compute_mean_and_ci(data, confidence_level=0.95):\n",
    "    mean_value = np.mean(data)\n",
    "    # Perform bootstrap resampling to compute confidence interval\n",
    "    res = bootstrap((data,), np.mean, confidence_level=confidence_level, n_resamples=10000, method='basic')\n",
    "    ci_lower, ci_upper = res.confidence_interval\n",
    "    return mean_value, ci_lower, ci_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for remote GODOT connection on port 14069\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [41], 'space': 'box'}}\n",
      "waiting for remote GODOT connection on port 12541\n",
      "connection established\n",
      "action space {'input': {'action_type': 'continuous', 'size': 4}}\n",
      "observation space {'obs': {'size': [41], 'space': 'box'}}\n",
      "Loaded_ 2x2_duck_r2_policy_Task_MHA_DQN__B_ACE_Eval250115-100402_69_BestRew.pth\n",
      "Loaded_ 2x2_duck_r2_policy_Task_MHA_DQN__B_ACE_Eval250115-100402_69_BestRew.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_33308\\2229624393.py:166: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy.policies[agent].load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "test_env_num = 1      \n",
    "seed = 1\n",
    "episodes =  100\n",
    "render  = False\n",
    "\n",
    "#Test Conditions\n",
    "Blue_test_agents = 2\n",
    "Red_test_agents = 2\n",
    "\n",
    "Enemies = [\"Duck\"]\n",
    "#Enemies = [\"FSM_B1\", \"FSM_G3\"]\n",
    "\n",
    "#Training Model selection\n",
    "Blue_training_agents = 2\n",
    "Red_training_agents = 2\n",
    "\n",
    "#Training_Bases = [\"Duck\", \"FSM_B1\",\"FSM_G3\", \"FSM_G10\"]\n",
    "Training_Bases = [\"Duck\"]\n",
    "#model == \"Task_DNN_B_ACE\"\n",
    "#model == \"Task_MHA_B_ACE\"\n",
    "\n",
    "# \"policy_Task_MHA_B_ACE_B_ACE03241024-165541_1185_BestRew\" / 1 x 2 FSMG10\n",
    "\n",
    "for Enemy_type in Enemies:\n",
    "    for Training_Enemy in Training_Bases:\n",
    "                        \n",
    "        if Blue_training_agents == 1 and Red_training_agents == 1:\n",
    "            \n",
    "            if Training_Enemy == \"Duck\":    \n",
    "                load_policy_name = f'1_DUCK_Training_policy_Task_MHA_B_ACE_B_ACE03241018-082438_1669_BestRew.pth'\n",
    "                TCode = 11\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_B1\":\n",
    "                load_policy_name = f'2_FSM_B1_Training_policy_Task_MHA_B_ACE_B_ACE03241017-210033_1301_BestRew.pth'\n",
    "                TCode = 21\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_G3\":            \n",
    "                load_policy_name = f'3_FSM_G3_Training_policy_Task_MHA_B_ACE_B_ACE03241019-215530_1893_BestRew.pth'\n",
    "                TCode = 31\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_G10\":            \n",
    "                load_policy_name = f'4_FSM_G10_policy_Task_MHA_B_ACE_B_ACE03241022-144023_1329_BestRew.pth'\n",
    "                TCode = 41\n",
    "        \n",
    "        elif Blue_training_agents == 1 and Red_training_agents == 2:\n",
    "            \n",
    "            if Training_Enemy == \"Duck\":    \n",
    "                #load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241018-082438_1669_BestRew.pth'\n",
    "                TCode = 12\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_B1\":\n",
    "                #load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241017-210033_1301_BestRew.pth'\n",
    "                TCode = 22\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_G3\":            \n",
    "                #load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241019-215530_1893_BestRew.pth'\n",
    "                TCode = 32\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_G10\":            \n",
    "                load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241024-165541_1185_BestRew.pth'\n",
    "                TCode = 42\n",
    "        \n",
    "        elif Blue_training_agents == 2 and Red_training_agents == 2:\n",
    "            \n",
    "            if Training_Enemy == \"Duck\":    \n",
    "                load_policy_name = f'2x2_duck_policy_Task_MHA_DQN__B_ACE_Eval241225-100957_725_BestRew.pth'\n",
    "                load_policy_name = f'2x2_duck2_policy_Task_MHA_DQN__B_ACE_Eval241225-100957_609_BestRew.pth'\n",
    "                load_policy_name = f'2x2_duck2_policy_Task_MHA_DQN__B_ACE_Eval241225-100957_1000_Step.pth'\n",
    "                load_policy_name = f'2x2_duck2_policy_Task_MHA_DQN__B_ACE_Eval241225-100957_609_BestRew.pth'\n",
    "                load_policy_name = f'2x2_duck_r2_policy_Task_MHA_DQN__B_ACE_Eval250115-100402_69_BestRew.pth'\n",
    "                TCode = 13\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_B1\":\n",
    "                load_policy_name = f'2x2_FSMB1_policy_Task_MHA_B_ACE_B_ACE03241118-103140_573_BestRew.pth'\n",
    "                TCode = 23\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_G3\":            \n",
    "                #load_policy_name = f'2x2_FSMG3_policy_Task_MHA_DQN__B_ACE_Eval241219-101521_197_BestRew.pth'\n",
    "                # load_policy_name = f'2x2_FSMG3_2_policy_Task_MHA_DQN__B_ACE_Eval241222-095741_293_BestRew.pth'\n",
    "                #load_policy_name = f'2x2_FSMG3_3_policy_Task_MHA_B_ACE_B_ACE03241121-090422_1_BestRew.pth'\n",
    "                load_policy_name = f'2x2_FSMG3_ret_policy_Task_MHA_DQN__B_ACE_Eval241231-190919_701_BestRew.pth'\n",
    "                TCode = 33\n",
    "            \n",
    "            elif Training_Enemy == \"FSM_G3_noRewShap\":            \n",
    "                # load_policy_name = f'2x2_FSMG3_policy_Task_MHA_DQN__B_ACE_Eval241219-101521_197_BestRew.pth'\n",
    "                # load_policy_name = f'2x2_FSMG3_2_policy_Task_MHA_DQN__B_ACE_Eval241222-095741_293_BestRew.pth'\n",
    "                # load_policy_name = f'2x2_FSMG3_3_policy_Task_MHA_B_ACE_B_ACE03241121-090422_1_BestRew.pth'\n",
    "                load_policy_name = f'2x2_FSMG3_noRewShap_policy_Task_MHA_DQN__B_ACE_Eval241228-144950_517_BestRew.pth'\n",
    "                TCode = 33\n",
    "                                \n",
    "            \n",
    "            elif Training_Enemy == \"FSM_G3_DNN\":            \n",
    "                load_policy_name = f'3.3.DNN.policy_Task_DNN_DQN__B_ACE_Eval241224-113618_589_BestRew.pth'\n",
    "                TCode = 330\n",
    "                \n",
    "            elif Training_Enemy == \"FSM_G10\":            \n",
    "                #load_policy_name = f'policy_Task_MHA_B_ACE_B_ACE03241024-165541_1185_BestRew.pth'\n",
    "                TCode = 43\n",
    "\n",
    "                \n",
    "        # Update B-ACE Enemy Config\n",
    "        if Enemy_type == \"Duck\":\n",
    "            new_enemy_behavior = \"duck\"   \n",
    "            new_mission = \"striker\"\n",
    "            new_dShot  = [1.04]\n",
    "            new_lCrank = [1.06]\n",
    "            new_lBreak = [1.05]\n",
    "             \n",
    "            ECode = 1\n",
    "            \n",
    "        elif Enemy_type == \"FSM_B1\":\n",
    "            new_enemy_behavior = \"baseline1\"\n",
    "            new_mission = \"striker\"\n",
    "            new_dShot  = [1.04]\n",
    "            new_lCrank = [1.06]\n",
    "            new_lBreak = [1.05]\n",
    "            ECode = 2\n",
    "            \n",
    "        elif Enemy_type == \"FSM_G3\":\n",
    "            new_enemy_behavior = \"baseline1\"\n",
    "            new_mission = \"striker\"\n",
    "           # Data for 3 agents\n",
    "            new_dShot = [0.50, 0.99, 1.04]\n",
    "            new_lCrank = [0.98, 0.96, 1.14]\n",
    "            new_lBreak = [1.17, 0.51, 1.05]                      \n",
    "\n",
    "            ECode = 3\n",
    "        \n",
    "        elif Enemy_type == \"FSM_G10\":\n",
    "            new_enemy_behavior = \"baseline1\"\n",
    "            new_mission = \"striker\"\n",
    "            # Data for 10 agents\n",
    "            new_dShot = [0.50, 0.99, 1.04, 0.50, 0.99, 0.93, 0.57, 0.50, 0.50, 0.50]\n",
    "            new_lCrank = [0.64, 0.96, 1.14, 0.69, 0.96, 0.69, 1.07, 0.20, 0.98, 0.69]\n",
    "            new_lBreak = [1.17, 0.51, 1.05, 0.25, 0.84, 0.51, 0.61, 0.37, 1.17, 0.51]            \n",
    "            ECode = 4\n",
    "\n",
    "\n",
    "        # Update the red agent's behavior\n",
    "        update_red_agent_behavior(\n",
    "            B_ACE_Config,\n",
    "            Blue_test_agents,\n",
    "            Red_test_agents, \n",
    "            new_enemy_behavior, \n",
    "            new_mission, \n",
    "            new_dShot, \n",
    "            new_lCrank, \n",
    "            new_lBreak\n",
    "        )\n",
    "        \n",
    "        policy, optim, agents = _get_agents()        \n",
    "        test_envs = DummyVectorEnv([_get_env for _ in range(test_env_num)])         \n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "                \n",
    "        \n",
    "        # Load the saved checkpoint\n",
    "        for agent in agents:    \n",
    "            \n",
    "            if Policy_Config[\"same_policy\"]:\n",
    "                model_path = os.path.join(\"Best_Behaviors\", load_policy_name)                            \n",
    "            else:\n",
    "                model_path = os.path.join(\"Best_Behaviors\", load_policy_name) \n",
    "\n",
    "            \n",
    "            policy.policies[agent].load_state_dict(torch.load(model_path))\n",
    "            policy.policies[agent].set_eps(0.01)\n",
    "            policy.policies[agent].eval()\n",
    "            \n",
    "            print(\"Loaded_\", load_policy_name)\n",
    "            \n",
    "        test_collector = Collector(policy, test_envs, exploration_noise=False)\n",
    "\n",
    "        results = test_collector.collect(n_episode=episodes)#0.02)#, gym_reset_kwargs={'seed' :2})\n",
    "\n",
    "        print(\"Mean: \", np.mean(results.returns))\n",
    "        print(\"Std:  \" , np.std (results.returns))\n",
    "        print(\"Max:  \" , np.max( results.returns))\n",
    "        print(\"Min:  \" , np.min(results.returns))\n",
    "\n",
    "        #Gets Final Stats\n",
    "        methods = test_envs.get_env_attr(\"call_results\")  # This returns a list of method references\n",
    "        results = [method() for method in methods]  # Call each method\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        data = results\n",
    "\n",
    "        blue_team_data = []\n",
    "        red_team_data = []\n",
    "        general_data = []\n",
    "\n",
    "        for episode in data:\n",
    "            for entry in episode:\n",
    "                blue_team_data.append(entry[0])  # First dictionary: Blue team\n",
    "                red_team_data.append(entry[1])   # Second dictionary: Red team\n",
    "                general_data.append(entry[2])    # Third dictionary: General sim data\n",
    "\n",
    "        # Convert to DataFrames\n",
    "        df_blue = pd.DataFrame(blue_team_data)\n",
    "        df_blue = df_blue[df_blue.index < episodes]\n",
    "        \n",
    "        df_blue['training_beh'] = Training_Enemy\n",
    "        df_blue['Blue_test_agents_num'] = Blue_test_agents\n",
    "        df_blue['Blue_training_agents_num'] = Blue_training_agents                \n",
    "\n",
    "        df_red = pd.DataFrame(red_team_data)\n",
    "        df_red = df_red[df_red.index < episodes]  \n",
    "        df_red['eval_enemy'] = Enemy_type      \n",
    "        df_blue['Red_test_agents_num'] = Red_test_agents\n",
    "        df_blue['Red_training_agents_num'] = Red_training_agents            \n",
    "\n",
    "        df_general = pd.DataFrame(general_data)\n",
    "\n",
    "        # Merge DataFrames for a complete view (optional)\n",
    "        df_merged = pd.concat([df_general, df_blue.add_prefix('blue_'), df_red.add_prefix('red_')], axis=1)\n",
    "        df_merged = df_merged[df_merged.blue_end_cond.notna()]\n",
    "\n",
    "        # Compute mean and confidence intervals for Blue and Red team metrics\n",
    "        metrics = ['killed', 'missile', 'mission', 'reward']\n",
    "\n",
    "        final_results = {'eval_enemy' : [], 'training_beh' : [], 'team': [], 'metric': [], 'mean': [], 'ci_lower': [], 'ci_upper': []}\n",
    "\n",
    "        for metric in metrics:\n",
    "            for team, df in [('Blue', df_blue), ('Red', df_red)]:\n",
    "                mean_value, ci_lower, ci_upper = compute_mean_and_ci(df[metric])\n",
    "                final_results['eval_enemy'].append(Enemy_type)\n",
    "                final_results['training_beh'].append(Training_Enemy)\n",
    "                final_results['team'].append(team)\n",
    "                final_results['metric'].append(metric)\n",
    "                final_results['mean'].append(mean_value)\n",
    "                final_results['ci_lower'].append(ci_lower)\n",
    "                final_results['ci_upper'].append(ci_upper)\n",
    "\n",
    "        # Convert the results into a DataFrame for display\n",
    "        df_final_results = pd.DataFrame(final_results)\n",
    "        print(df_final_results)\n",
    "\n",
    "        df_merged.to_csv(f'{Blue_test_agents}x{Red_test_agents}_{TCode}.{ECode}.Merged_Eval_{Training_Enemy}_Training_{Enemy_type}_Enemy.csv')\n",
    "        df_final_results.to_csv(f'{TCode}.{ECode}.Final_Eval_{Training_Enemy}_Training_{Enemy_type}_Enemy.csv')\n",
    "        \n",
    "        # Clean up and close the environments\n",
    "        #test_collector.reset()\n",
    "        #test_envs.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_collector.collect(n_episode=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b_ace_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
